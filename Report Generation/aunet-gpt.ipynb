{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12230251,"sourceType":"datasetVersion","datasetId":7705869},{"sourceId":12271056,"sourceType":"datasetVersion","datasetId":7732826},{"sourceId":443559,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":360226,"modelId":381366}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"toc_visible":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:10:51.068265Z","iopub.execute_input":"2025-06-20T16:10:51.068616Z","iopub.status.idle":"2025-06-20T16:10:53.584608Z","shell.execute_reply.started":"2025-06-20T16:10:51.068590Z","shell.execute_reply":"2025-06-20T16:10:53.583782Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"7i3_UeP9yy02","executionInfo":{"status":"ok","timestamp":1750783036190,"user_tz":-330,"elapsed":391,"user":{"displayName":"Rahul Bhardwaj","userId":"02352809289654515916"}}},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import cv2\n","import matplotlib.pyplot as plt\n","import os\n","\n","# ================ ATTENTION UNET MODEL ================\n","class conv_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.conv = conv_block(in_c, out_c)\n","        self.pool = nn.MaxPool2d((2, 2))\n","\n","    def forward(self, x):\n","        s = self.conv(x)\n","        p = self.pool(s)\n","        return s, p\n","\n","class attention_gate(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.Wg = nn.Sequential(\n","            nn.Conv2d(in_c[0], out_c, kernel_size=1, padding=0),\n","            nn.BatchNorm2d(out_c)\n","        )\n","        self.Ws = nn.Sequential(\n","            nn.Conv2d(in_c[1], out_c, kernel_size=1, padding=0),\n","            nn.BatchNorm2d(out_c)\n","        )\n","        self.relu = nn.ReLU(inplace=True)\n","        self.output = nn.Sequential(\n","            nn.Conv2d(out_c, out_c, kernel_size=1, padding=0),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, g, s):\n","        Wg = self.Wg(g)\n","        Ws = self.Ws(s)\n","        out = self.relu(Wg + Ws)\n","        out = self.output(out)\n","        return out * s\n","\n","class decoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.ag = attention_gate(in_c, out_c)\n","        self.c1 = conv_block(in_c[0]+out_c, out_c)\n","\n","    def forward(self, x, s):\n","        x = self.up(x)\n","        s = self.ag(x, s)\n","        x = torch.cat([x, s], dim=1)\n","        x = self.c1(x)\n","        return x\n","\n","class attention_unet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # Change input channels from 3 to 1 for grayscale images\n","        self.e1 = encoder_block(1, 64)  # Modified line\n","        self.e2 = encoder_block(64, 128)\n","        self.e3 = encoder_block(128, 256)\n","        self.b1 = conv_block(256, 512)\n","        self.d1 = decoder_block([512, 256], 256)\n","        self.d2 = decoder_block([256, 128], 128)\n","        self.d3 = decoder_block([128, 64], 64)\n","        self.output = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n","\n","        # Add final activation\n","        # self.final_activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        s1, p1 = self.e1(x)\n","        s2, p2 = self.e2(p1)\n","        s3, p3 = self.e3(p2)\n","\n","        b1 = self.b1(p3)\n","        d1 = self.d1(b1, s3)\n","        d2 = self.d2(d1, s2)\n","        d3 = self.d3(d2, s1)\n","        output = self.output(d3)\n","        return output\n","\n","\n","# ================== STRUCTURED REPORT GENERATOR ==================\n","class StructuredReportGenerator:\n","    def __init__(self, unet_model_path, device='cpu', pixel_to_mm=0.1):\n","        self.device = torch.device(device)\n","        self.unet = self._load_unet(unet_model_path)\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.Grayscale(num_output_channels=1),\n","            transforms.ToTensor(),\n","        ])\n","        self.pixel_to_mm = pixel_to_mm\n","\n","    def _load_unet(self, model_path):\n","        model = attention_unet()\n","        state_dict = torch.load(model_path, map_location=self.device)\n","\n","        # Handle DataParallel wrapper\n","        new_state_dict = {}\n","        for k, v in state_dict.items():\n","            if k.startswith('module.'):\n","                new_state_dict[k[7:]] = v\n","            else:\n","                new_state_dict[k] = v\n","\n","        model.load_state_dict(new_state_dict)\n","        model.eval().to(self.device)\n","        return model\n","\n","    def predict_mask(self, image_tensor, threshold=0.5):\n","        with torch.no_grad():\n","            output = self.unet(image_tensor.unsqueeze(0).to(self.device))\n","            mask = torch.sigmoid(output).cpu().squeeze(0).squeeze(0).numpy()\n","            return (mask > threshold).astype(np.uint8) * 255\n","\n","    def extract_features(self, original_img, mask):\n","        orig_img_array = np.array(original_img)\n","        binary_mask = (mask > 0).astype(np.uint8)\n","\n","        # Resize mask to match original dimensions\n","        orig_h, orig_w = orig_img_array.shape\n","        resized_mask = cv2.resize(\n","            binary_mask,\n","            (orig_w, orig_h),\n","            interpolation=cv2.INTER_NEAREST\n","        )\n","\n","        features = {}\n","        contours, _ = cv2.findContours(\n","            resized_mask,\n","            cv2.RETR_EXTERNAL,\n","            cv2.CHAIN_APPROX_SIMPLE\n","        )\n","\n","        if contours:\n","            largest_contour = max(contours, key=cv2.contourArea)\n","            pixel_area = cv2.contourArea(largest_contour)\n","            features['area_mm'] = pixel_area * (self.pixel_to_mm ** 2)\n","\n","            x, y, w, h = cv2.boundingRect(largest_contour)\n","            features['aspect_ratio'] = w / max(h, 1e-5)\n","\n","            masked_region = orig_img_array * resized_mask\n","            pixels = masked_region[resized_mask > 0]\n","\n","            if pixels.size > 0:\n","                mean_intensity = np.mean(pixels)\n","                if mean_intensity < 100:\n","                    features['echogenicity'] = \"hypoechoic\"\n","                elif mean_intensity < 180:\n","                    features['echogenicity'] = \"isoechoic\"\n","                else:\n","                    features['echogenicity'] = \"hyperechoic\"\n","            else:\n","                features['echogenicity'] = \"undetermined\"\n","\n","        return features\n","\n","    def generate_structured_report(self, features):\n","        birads_category = \"Category 4: Suspicious abnormality - biopsy recommended\"\n","        if features.get('aspect_ratio', 0) < 1.4 and features.get('area_mm', 0) < 50:\n","            birads_category = \"Category 3: Probably benign - short-term follow-up suggested\"\n","\n","        recommendations = \"Ultrasound-guided core needle biopsy for histopathological correlation.\"\n","        if \"Category 3\" in birads_category:\n","            recommendations = \"Follow-up ultrasound in 6 months recommended.\"\n","\n","        return (\n","            \"1. Clinical Findings: \"\n","            f\"Irregular {features.get('echogenicity', 'hypoechoic')} mass \"\n","            f\"measuring {features.get('area_mm', 0):.2f} mmÂ² with spiculated margins.\\n\"\n","            \"2. Morphological Description: \"\n","            f\"Lesion demonstrates irregular contours (aspect ratio: {features.get('aspect_ratio', 0):.2f}) \"\n","            \"and heterogeneous internal echotexture.\\n\"\n","            \"3. Echogenicity Assessment: \"\n","            f\"Predominantly {features.get('echogenicity', 'hypoechoic')} \"\n","            \"with focal hyperechoic components.\\n\"\n","            f\"4. BI-RADS Classification: {birads_category}\\n\"\n","            f\"5. Clinical Recommendations: {recommendations}\"\n","        )\n","\n","    def process_scan(self, image_path, threshold=0.5):\n","        original_img = Image.open(image_path).convert('L')\n","        img_tensor = self.transform(original_img)\n","        mask = self.predict_mask(img_tensor, threshold)\n","        features = self.extract_features(original_img, mask)\n","        report = self.generate_structured_report(features)\n","\n","        return {\n","            \"original_scan\": np.array(original_img),\n","            \"segmentation_mask\": mask,\n","            \"structured_report\": report\n","        }\n","\n","    def display_results(self, image_path, threshold=0.5):\n","        results = self.process_scan(image_path, threshold)\n","\n","        plt.figure(figsize=(12, 6))\n","\n","        plt.subplot(1, 2, 1)\n","        plt.imshow(results['original_scan'], cmap='gray')\n","        plt.title('Original Scan')\n","        plt.axis('off')\n","\n","        plt.subplot(1, 2, 2)\n","        plt.imshow(results['segmentation_mask'], cmap='gray')\n","        plt.title('Segmentation Mask')\n","        plt.axis('off')\n","\n","        # plt.subplot(1, 3, 3)\n","        # plt.text(0.05, 0.5, results['structured_report'],\n","        #          fontsize=10, ha='left', va='center', wrap=True)\n","        # plt.axis('off')\n","        # plt.title('Structured Report')\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","        return results\n","\n","# ================ USAGE EXAMPLE ================\n","if __name__ == \"__main__\":\n","    report_generator = StructuredReportGenerator(\n","        unet_model_path='/kaggle/input/aunet/pytorch/default/1/attention_unet_busi.pth',\n","        pixel_to_mm=0.1,\n","        device='cuda' if torch.cuda.is_available() else 'cpu'\n","    )\n","\n","    scan_path = '/kaggle/input/busi-malignant/malignant/malignant (108).png'\n","    results = report_generator.display_results(scan_path)\n","\n","    print(\"\\n\" + \"=\"*40)\n","    print(\"STRUCTURED REPORT\".center(40))\n","    print(\"=\"*40)\n","    print(results['structured_report'])\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:01:30.572191Z","iopub.execute_input":"2025-06-24T16:01:30.572869Z","iopub.status.idle":"2025-06-24T16:01:31.059499Z","shell.execute_reply.started":"2025-06-24T16:01:30.572821Z","shell.execute_reply":"2025-06-24T16:01:31.058512Z"},"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"FZuOgM8xyy03","executionInfo":{"status":"error","timestamp":1750783051608,"user_tz":-330,"elapsed":12828,"user":{"displayName":"Rahul Bhardwaj","userId":"02352809289654515916"}},"outputId":"64a62960-19fb-499a-9cba-d629d3564ec2"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/aunet/pytorch/default/1/attention_unet_busi.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-912430118.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;31m# ================ USAGE EXAMPLE ================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     report_generator = StructuredReportGenerator(\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0munet_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/kaggle/input/aunet/pytorch/default/1/attention_unet_busi.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mpixel_to_mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2-912430118.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, unet_model_path, device, pixel_to_mm)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munet_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_to_mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         self.transform = transforms.Compose([\n\u001b[1;32m    114\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2-912430118.py\u001b[0m in \u001b[0;36m_load_unet\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Handle DataParallel wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/aunet/pytorch/default/1/attention_unet_busi.pth'"]}],"execution_count":2},{"cell_type":"code","source":["!pip install sacremoses\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T14:34:27.038929Z","iopub.execute_input":"2025-06-24T14:34:27.039422Z","iopub.status.idle":"2025-06-24T14:34:31.065758Z","shell.execute_reply.started":"2025-06-24T14:34:27.039401Z","shell.execute_reply":"2025-06-24T14:34:31.064839Z"},"id":"vUO1_I5-yy05","outputId":"6a2bdf5f-0a43-4cdb-b4b6-df904cee4188"},"outputs":[{"name":"stdout","text":"Collecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n","output_type":"stream"}],"execution_count":null}]}